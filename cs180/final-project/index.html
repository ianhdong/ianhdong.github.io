<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/cs180/css/styles.css">
    <script src="/cs180/js/hamburger.js" defer></script>


    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>CS 180 Final Project: Light Field Cameras and Gradient Domain Fusion</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

</head>


<body>
    <div class="content">

        <h1 align="middle">CS 180: Intro to Computer Vision and Computational Photography, Fall 2024</h1>
        <h1 align="middle"><a href="#lightfield-cameras">Final Project: Lightfield Cameras</a>
        </h1>
        <h2 align="middle">Ian Dong</h2>

        <br><br>



        <div class="bounding-box">

            <h2 align="middle" id="lightfield-cameras">Overview</h2>

            For this project, I reproduced the effects from a Lytro camera using real lightfield data via shifting and
            averaging. The images were captured over a plane orthogonal to the optical axis which helps to achieve the
            complex effects shown below.
            <br><br>
        </div>

        <br><br>

        <h2 align="middle">Section I: Depth Refocusing</h2>

        <div class="bounding-box">
            <h3 align="middle">
                Depth Refocusing
            </h3>
            <p>
                For this part of the project, I took in all of the images and found the center one as the reference
                image. The naive method would be to simply average these images without any shifting. This, however,
                will produce an image which is sharp around the far-away objects but blurry around the nearby ones. To
                address this, I shifted the images appropriately around the reference image and then used a constant to
                vary this shift. This is the following equation I used:

                $$ \text{(shift_x, shift_y)} = c \cdot (\text{(reference_x, reference_y)} - \text{(center_x, center_y)})$$

                Here are some images and gif with varying levels of $c$.
            </p>
            <div align="middle"></div>
            <table>
                <tr align="center">
                    <td>
                        <img src="./Images/light-field/depth-refocusing/depth-refocus-0.15.png" align="middle"
                            width="400px" />
                        <figcaption>$c = 0.15$</figcaption>
                    </td>
                    <td>
                        <img src="./Images/light-field/depth-refocusing/depth-refocus-0.5.png" align="middle"
                            width="400px" />
                        <figcaption>$c = 0.5$</figcaption>
                    </td>
                </tr>
            </table>

            <table>
                <tr align="center">
                    <td>
                        <img src="./Images/light-field/depth-refocusing/depth_refocusing.gif" align="middle"
                            width="400px" />
                        <figcaption>Gif for $c \in [-0.5, 0.5]$</figcaption>
                    </td>
                </tr>
            </table>

            <p>
                As shown in the gif, the depth gets refocused on different parts of the image based on the value of $c$.
                The far-away objects become blurry when $c$ is negative and the nearby objects become blurry when $c$ is
                positive.
            </p>
        </div>

        <br>
        <br>

        <h2 align="middle">Section II: Aperture Adjustment</h2>
        <div class="bounding-box">
            <h3 align="middle">
                Aperture Adjustment
            </h3>
            <p>
                In this section, I simulated changing the aperture which affects the focused image regions. While
                maintaining the same $c$ value, I varied the radius away from the center and used only those images
                within the circle for my final averaged image. Although I used fewer images compared to before, the
                resulting image was more focused in the center. Here are some of the images and gif with varying radii.
            </p>

            <table>
                <tr align="center">
                    <td>
                        <img src="./Images/light-field/aperture-adjustment/aperture-adjustment-2.png" align="middle"
                            width="400px" />
                        <figcaption>Radius = 2</figcaption>
                    </td>
                    <td>
                        <img src="./Images/light-field/aperture-adjustment/aperture-adjustment-6.png" align="middle"
                            width="400px" />
                        <figcaption>Radius = 6</figcaption>
                    </td>
                </tr>
            </table>
            <table>
                <tr align="center">
                    <td>
                        <img src="./Images/light-field/aperture-adjustment/aperture_adjustment.gif" align="middle"
                            width="400px" />
                        <figcaption>Gif for Varying Radius</figcaption>
                    </td>
                </tr>
            </table>
            <br>
            <p>
                As shown in the gif, the center pieces of the image are in focus while the outer pieces shift in their blurriness. This demonstrates the effect of changing the aperture.
            </p>
        </div>
        <br>
        <br>
        <h2 align="middle">Section III: Summary</h2>
        <div class="bounding-box">
            <h3 align="middle">
                Learnings
            </h3>
            <p>
                It was very interesting to see how I could simulate the effects of a Lytro camera using real lightfield
                data. I learned how to shift and average images to achieve the desired effects and how to vary the shift
                to get different results. I also learned how to simulate changing the aperture to focus on different
                parts of the image.
            </p>
        </div>
        <br>
        <br>

        <h1 align="middle"><a href="#gradient-domain-fusion">Final Project: Gradient Domain Fusion</a>
        </h1>

        <div class="bounding-box">

            <h2 align="middle" id="gradient-domain-fusion">Overview</h2>

            The second project that I worked on explored gradient-domain processing and how it can be used to seamlessly
            blend an object or texture from a source image into a target image. I used the Poisson blending algorithm to
            achieve this effect. The images below show the source and target images as well as the final blended image.
            <br><br>
        </div>

        <br>
        <br>

        <h2 align="middle">Section I: Toy Problem</h2>

        <div class="bounding-box">
            <h3 align="middle">
                Toy Problem
            </h3>
            <p>
                In the first part of the project, I found the best pixels to reconstruct a toy problem. The insight is
                that we care much more about the gradient of an image instead of the overall intensity. This means we
                can set up the problem as finding values for the target pixels that maximally preserve the gradient of
                the source region without changing any of the background pixels. I formulated this objective as a least
                squares problem and tried to minimize the following three objectives.

                $$
                \begin{align*}
                (v(x + 1, y) - v(x, y) &- ((s(x + 1, y) - s(x, y))))^2 \\
                (v(x, y + 1) - v(x, y) &- ((s(x, y + 1) - s(x, y))))^2 \\
                (v(1, 1) &- s(1, 1))^2
                \end{align*}
                $$
                where $x$ and $y$ are the gradients from an image $s$ and $v$ is the reconstructed resulting image. For
                the toy problem, I was able to solve this least squares problem to recover the original image.
            </p>
            <table>
                <tr align="center">
                    <td>
                        <img src="./Images/gradient-domain-fusion/toy-problem/toy-original.png" align="middle" />
                        <figcaption>Toy Story (Original)</figcaption>
                    </td>
                    <td>
                        <img src="./Images/gradient-domain-fusion/toy-problem/toy-reconstructed.png" align="middle" />
                        <figcaption>Toy Story (Reconstructed)</figcaption>
                    </td>
                </tr>
            </table>
            <br>
            <p>
                As shown above, the reconstructed image is very similar to the original image. Both are a bit grainy
                because they were small and blown up to be easily viewed.
            </p>
        </div>

        <br>
        <br>

        <h2 align="middle">Section II: Poisson Blending</h2>

        <div class="bounding-box">
            <h3 align="middle">
                Poisson Blending
            </h3>
            <p>
                For the next part, I worked on Poisson blending. This was very similar to the above toy problem but instead focused on the pixels inside of the mask. First, I chose an image to put into the foreground of my original image. Since the interactive drawing tool wasn't quite working, I manually created the mask to what I wanted the final image to look like. I also passed in the original image because I wanted to make sure that the background of the final image was the same as the original image outside of where I placing the mask. Let $f$ be the final image, $t$ be the background image, and $s$ be the foreground image where we are trying to place $s$ into $t$ to create $f$. There were three types of equations to solve:
                <ul>
                    <li>
                        If the pixel in the final image $f$ is outside of the mask, take the pixel from the target image $t$.
                        <ul>
                            <li>
                                $f[i, j] = t[i, j]$
                            </li>
                        </ul>
                    </li>

                    <li>
                        If the pixel and its four neighbors in the final image $f$ are all inside of the mask, try to match the gradients in $f$ with the gradients in the source image $s$.
                        <ul>
                            <li>
                                $f[i, j] - f[i - 1, j] = s[i, j] - s[i - 1, j]$
                            </li>
                            <li>
                                $f[i, j] - f[i + 1, j] = s[i, j] - s[i + 1, j]$
                            </li>
                            <li>
                                $f[i, j] - f[i, j - 1] = s[i, j] - s[i, j - 1]$
                            </li>
                            <li>
                                $f[i, j] - f[i, j + 1] = s[i, j] - s[i, j + 1]$
                            </li>
                        </ul>
                    </li>

                    <li>
                        If the pixel and its four neighbors in the final image $f$ are all inside of the mask, try to match the gradients in $f$ with the gradients in the source image $s$.
                        <ul>
                            <li>
                                $f[i, j] - f[i - 1, j] = s[i, j] - s[i - 1, j]$
                            </li>
                            <li>
                                $f[i, j] - f[i + 1, j] = s[i, j] - s[i + 1, j]$
                            </li>
                            <li>
                                $f[i, j] - f[i, j - 1] = s[i, j] - s[i, j - 1]$
                            </li>
                            <li>
                                $f[i, j] - f[i, j + 1] = s[i, j] - s[i, j + 1]$
                            </li>
                        </ul>
                    </li>

                    <li>
                        If the pixel and its four neighbors in the final image $f$ are not all inside of the mask, try to match the gradients for $f$ and $t$ with the gradients in $s$
                        <ul>
                            <li>
                                $f[i, j] - t[i - 1, j] = s[i, j] - s[i - 1, j]$
                            </li>
                            <li>
                                $f[i, j] - t[i + 1, j] = s[i, j] - s[i + 1, j]$
                            </li>
                            <li>
                                $f[i, j] - t[i, j - 1] = s[i, j] - s[i, j - 1]$
                            </li>
                            <li>
                                $f[i, j] - t[i, j + 1] = s[i, j] - s[i, j + 1]$
                            </li>
                        </ul>
                    </li>

                </ul>
        
            </p>
            <table>
                <tr align="center">
                    <td>
                        <img src="./Images/gradient-domain-fusion/toy-problem/toy-original.png" align="middle" />
                        <figcaption>Toy Story (Original)</figcaption>
                    </td>
                    <td>
                        <img src="./Images/gradient-domain-fusion/toy-problem/toy-reconstructed.png" align="middle" />
                        <figcaption>Toy Story (Reconstructed)</figcaption>
                    </td>
                </tr>
            </table>
            <br>
        </div>
</body>

</html>